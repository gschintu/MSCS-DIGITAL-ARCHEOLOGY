{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Code and Functions\n",
    "Run this first to import modules and global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Models\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Global Functions\n",
    "\n",
    "def get_top50_ann(target_embedding, embeddings):\n",
    "    nn = NearestNeighbors(n_neighbors=51, metric='cosine', algorithm='brute')\n",
    "    nn.fit(embeddings)\n",
    "    distances, indices = nn.kneighbors([target_embedding])\n",
    "    return indices[0][1:]  # Skip the first index because it's the target itself\n",
    "\n",
    "def get_top50_euclidean(target_embedding, embeddings):\n",
    "    distances = [distance.euclidean(target_embedding, emb) for emb in embeddings]\n",
    "    indices = np.argsort(distances)[1:51]  # Skip the first index because it's the target itself\n",
    "    return indices\n",
    "\n",
    "def display_image(index):\n",
    "    display(Image(filename=f\"thumbnails_folder2large/{g_movie_embeddings[index]['input']}\"))\n",
    "\n",
    "def display_images(indices, embeddings):\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(20, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(plt.imread(f\"thumbnails_folder2large/{g_movie_embeddings[indices[i]]['input']}\"))\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images_first_x_last_x(indices, first_x, last_x, cluster_n=0):\n",
    "    # Select the first_x and last_x indices\n",
    "    selected_indices = indices[:first_x] + indices[-last_x:]\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplot\n",
    "    total_images = first_x + last_x\n",
    "    cols = 10\n",
    "    rows = math.ceil(total_images / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 2 * rows))\n",
    "    axes = axes.ravel()  # Flatten the axes array\n",
    "    \n",
    "    # Hide all axes\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Display images on the first len(selected_indices) axes\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        axes[i].imshow(plt.imread(f\"thumbnails_folder2large/{g_movie_embeddings[idx]['input']}\"))\n",
    "        axes[i].axis('on')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"Cluster {cluster_n} -  First {first_x} and Last {last_x} Images - Total Images in Cluster: {len(indices)}\")\n",
    "    plt.show()\n",
    "\n",
    "def display_cluster_images(cluster_labels, cluster_number):\n",
    "    # Get indices of images in the cluster\n",
    "    indices = [i for i, label in enumerate(cluster_labels) if label == cluster_number]\n",
    "    \n",
    "    # Display images\n",
    "    display_images(indices)\n",
    "\n",
    "def display_cluster_images_first_last_x(cluster_labels, cluster_number, first_x, last_x):\n",
    "    # Get indices of images in the cluster\n",
    "    indices = [i for i, label in enumerate(cluster_labels) if label == cluster_number]\n",
    "    \n",
    "    # Display images\n",
    "    display_images_first_x_last_x(indices, first_x, last_x, cluster_number)\n",
    "\n",
    "def find_and_remove_intro_and_subtitles(g_only_embeddings, threshold=0.7):\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # Get the text embeddings for the intro and subtitles\n",
    "    inputs = processor(text=[\"Image of Walt Disney Movie Intro\", \"Walt Disney Movie Intro\", \"Movie closing credits\", \"Movie end credits\", \"Image with lots of closing credits\"], return_tensors=\"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    text_embeddings_np = text_embeddings.detach().numpy()\n",
    "\n",
    "    # Calculate the cosine similarity between the text embeddings and the movie embeddings\n",
    "    similarities = cosine_similarity(text_embeddings_np, g_only_embeddings)\n",
    "\n",
    "    # Find the indices of the embeddings that are similar to the intro and subtitles\n",
    "    intro_subtitle_indices = np.where(similarities.max(axis=0) > threshold)[0]\n",
    "\n",
    "    print(\"Number of images(Intro and Closing credits) to remove:\", intro_subtitle_indices)\n",
    "\n",
    "    # Remove the intro and subtitles from the movie embeddings\n",
    "    for idx in sorted(intro_subtitle_indices, reverse=True):\n",
    "        del g_movie_embeddings[idx]\n",
    "        g_only_embeddings = np.delete(g_only_embeddings, idx, axis=0)\n",
    "    \n",
    "\n",
    "# Global Variables\n",
    "\n",
    "g_movie_embeddings = json.load(open(\"honey_i_shrunk_the_kids_movie_embeddings_1_second.json\"))\n",
    "g_only_embeddings = np.array([emb['embedding'] for emb in g_movie_embeddings])\n",
    "find_and_remove_intro_and_subtitles(g_only_embeddings, threshold=0.237)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "\n",
    "We will compare clustering with t-SNE (t-Distributed Stochastic Neighbor Embedding) and PCA (Principal Component Analysis) dimensionality reduction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using t-SNE to embed the vectors into 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tSNE_embedded_vectors = tsne.fit_transform(g_only_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA to embed the vectors into 2D\n",
    "pca = PCA(n_components=2)\n",
    "PCA_embedded_vectors = pca.fit_transform(g_only_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster t-SNE and PCA with K-Means and display Silhoutte Score\n",
    "\n",
    "**Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the best clustering number for t-SNE and PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tSNE_embedded_vectors\n",
    "\n",
    "# Range of clusters to try\n",
    "num_clusters = range(2, 20)\n",
    "\n",
    "# List to hold silhouette scores\n",
    "sil_scores = []\n",
    "\n",
    "# Loop over number of clusters\n",
    "for k in num_clusters:\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_init=\"auto\", n_clusters=k, random_state=42).fit(X)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute silhouette score and append to list\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(num_clusters, sil_scores, 'bx-')\n",
    "plt.title('t-SNE')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X = PCA_embedded_vectors\n",
    "\n",
    "# Range of clusters to try\n",
    "num_clusters = range(2, 20)\n",
    "\n",
    "# List to hold silhouette scores\n",
    "sil_scores = []\n",
    "\n",
    "# Loop over number of clusters\n",
    "for k in num_clusters:\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_init=\"auto\", n_clusters=k, random_state=42).fit(X)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute silhouette score and append to list\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(num_clusters, sil_scores, 'bx-')\n",
    "plt.title('PCA')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets cluster with t-SNE and PCA best Silhoutte Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing KMeans clustering with best k silhoutte score.\n",
    "kmeans = KMeans(n_init=\"auto\", n_clusters=16, random_state=42)\n",
    "tSNE_clusters = kmeans.fit_predict(tSNE_embedded_vectors)\n",
    "\n",
    "kmeans = KMeans(n_init=\"auto\", n_clusters=4, random_state=42)\n",
    "PCA_clusters = kmeans.fit_predict(PCA_embedded_vectors)\n",
    "\n",
    "\n",
    "# Extracting numbers from file names for labels\n",
    "labels = [re.search(r'\\d+', vector['input']).group() for vector in g_movie_embeddings]\n",
    "\n",
    "#t-SNE\n",
    "# Plotting the embedded vectors with cluster coloring\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "sns.scatterplot(x=tSNE_embedded_vectors[:, 0], y=tSNE_embedded_vectors[:, 1], hue=tSNE_clusters, palette='bright', legend='full', s=100)\n",
    "for i, vec in enumerate(tSNE_embedded_vectors):\n",
    "    plt.text(vec[0] + 0.02, vec[1] + 0.02, labels[i], fontsize=6)  # Adding labels\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('t-SNE Embedded Vectors with KMeans Clustering (k=16)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "#PCA\n",
    "# Plotting the embedded vectors with cluster coloring\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "sns.scatterplot(x=PCA_embedded_vectors[:, 0], y=PCA_embedded_vectors[:, 1], hue=PCA_clusters, palette='bright', legend='full', s=100)\n",
    "for i, vec in enumerate(PCA_embedded_vectors):\n",
    "    plt.text(vec[0] + 0.02, vec[1] + 0.02, labels[i], fontsize=6)  # Adding labels\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('PCA Embedded Vectors with KMeans Clustering (k=4)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let sample images in t-SNE clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = set(tSNE_clusters)\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    display_cluster_images_first_last_x(tSNE_clusters, cluster, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets sample images in PCA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = set(PCA_clusters)\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    display_cluster_images_first_last_x(PCA_clusters, cluster, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cluster_images_first_last_x(tSNE_clusters, cluster, 192, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to timestamps by dividing by frame rate\n",
    "# Assuming `frame_rate` is the frame rate of the movie\n",
    "frame_rate = 24\n",
    "timestamps = [int(label) / frame_rate for label in labels]\n",
    "\n",
    "# Create a timeline plot for the t-SNE clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps, tSNE_clusters, c=tSNE_clusters, cmap='viridis')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster')\n",
    "plt.title('t-SNE Clusters Over Time')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Create a timeline plot for the PCA clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps, PCA_clusters, c=PCA_clusters, cmap='viridis')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster')\n",
    "plt.title('PCA Clusters Over Time')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Film Description\n",
    "\n",
    "In 'Honey, I Shrunk the Kids,' an eccentric inventor, Wayne Szalinski, accidentally shrinks his and his neighbor's children with his experimental shrink ray. The miniature kids must navigate a perilous journey across their now-gigantic backyard, encountering obstacles like insects and sprinklers, as they try to return home.\n",
    "\n",
    "The film is notable for its creative visual effects that magnify ordinary environments into epic landscapes. It's a blend of adventure, humor, and family dynamics, ultimately showcasing the children's resourcefulness and the parents' determination to rescue their kids. The movie was a commercial success and spawned a franchise including sequels and a television series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Summary\n",
    "\n",
    "This section should highlight methods you used in your exploratory analysis. You should include at least one clustering technique or develop another way to relate frames to other frames. You should also consider dimensionality reduction.\n",
    "\n",
    "Each thumbnail is one frame of the Movie. Each thumbnail has been analyzed and embedded via CLIP. Referenced Model: https://replicate.com/andreasjansson/clip-features/examples .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunches and Hypotheses\n",
    "\n",
    "This section should summarize the questions that you asked about the film that could potentially be answered by exploratory analysis. You should ask at least three questions.\n",
    "\n",
    "**Hypothesis 1** \n",
    "-   Using Clip to find an object and observe ~5 frames before and after and interpret the results.\n",
    "\n",
    "**Hypothesis 2** \n",
    "-   Scene Consistency and Transition - Frames that are visually and thematically similar cluster together tightly in t-SNE and PCA visualizations, and distinct clusters correspond to different scenes or settings in the movie.\n",
    "-  **Rationale:** This hypothesis tests the ability of CLIP embeddings, which capture both visual and semantic content, to differentiate between distinct scenes based on their visual content and thematic elements.\n",
    "\n",
    "**Hypothesis 3**\n",
    "-   Find a model that can analyze an image and identify how many objects are there. 2 trees, 1 dog, 3 humans, a house...etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Interpretation\n",
    "\n",
    "**Hypothesis 1:**\n",
    "\n",
    "**Hypothesis 2:** \n",
    "- After removing closing credits frames and applying t-SNE and K-means we examined samples from identified clusters and we can observe that frames correspond to specific scenes or types of scenes (indoor vs outdoor, calm vs action-packed). Embeddings effectively capture scene-specific features and can be used to segment the movie based on visual content. Also, clusters seem to reflect the proper timeframe and transition of the movie.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Reflect on your process of analysis. What worked well and did not work well? Describe the limitations of the work and describe what you would work on with more time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
