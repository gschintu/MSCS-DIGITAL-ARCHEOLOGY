{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Code and Functions\n",
    "Run this first to import modules and global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Models\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Global Functions\n",
    "\n",
    "def get_top50_ann(target_embedding, embeddings):\n",
    "    nn = NearestNeighbors(n_neighbors=51, metric='cosine', algorithm='brute')\n",
    "    nn.fit(embeddings)\n",
    "    distances, indices = nn.kneighbors([target_embedding])\n",
    "    return indices[0][1:]  # Skip the first index because it's the target itself\n",
    "\n",
    "def get_top50_euclidean(target_embedding, embeddings):\n",
    "    distances = [distance.euclidean(target_embedding, emb) for emb in embeddings]\n",
    "    indices = np.argsort(distances)[1:51]  # Skip the first index because it's the target itself\n",
    "    return indices\n",
    "\n",
    "def display_image(index):\n",
    "    display(Image(filename=f\"thumbnails_folder2large/{g_movie_embeddings[index]['input']}\"))\n",
    "\n",
    "def display_images(indices, embeddings):\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(20, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(plt.imread(f\"thumbnails_folder2large/{g_movie_embeddings[indices[i]]['input']}\"))\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images_first_x_last_x(indices, first_x, last_x, cluster_n=0):\n",
    "    # Select the first_x and last_x indices\n",
    "    selected_indices = indices[:first_x] + indices[-last_x:]\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplot\n",
    "    total_images = first_x + last_x\n",
    "    cols = 10\n",
    "    rows = math.ceil(total_images / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 2 * rows))\n",
    "    axes = axes.ravel()  # Flatten the axes array\n",
    "    \n",
    "    # Hide all axes\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Display images on the first len(selected_indices) axes\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        axes[i].imshow(plt.imread(f\"thumbnails_folder2large/{g_movie_embeddings[idx]['input']}\"))\n",
    "        axes[i].axis('on')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"Cluster {cluster_n} -  First {first_x} and Last {last_x} Images - Total Images in Cluster: {len(indices)}\")\n",
    "    plt.show()\n",
    "\n",
    "def display_cluster_images(cluster_labels, cluster_number):\n",
    "    # Get indices of images in the cluster\n",
    "    indices = [i for i, label in enumerate(cluster_labels) if label == cluster_number]\n",
    "    \n",
    "    # Display images\n",
    "    display_images(indices)\n",
    "\n",
    "def display_cluster_images_first_last_x(cluster_labels, cluster_number, first_x, last_x):\n",
    "    # Get indices of images in the cluster\n",
    "    indices = [i for i, label in enumerate(cluster_labels) if label == cluster_number]\n",
    "    \n",
    "    # Display images\n",
    "    display_images_first_x_last_x(indices, first_x, last_x, cluster_number)\n",
    "\n",
    "def find_and_remove_intro_and_subtitles(g_only_embeddings, threshold=0.7):\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # Get the text embeddings for the intro and subtitles\n",
    "    inputs = processor(text=[\"Image of Walt Disney Movie Intro\", \"Walt Disney Movie Intro\", \"Movie closing credits\", \"Movie end credits\", \"Image with lots of closing credits\"], return_tensors=\"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    text_embeddings_np = text_embeddings.detach().numpy()\n",
    "\n",
    "    # Calculate the cosine similarity between the text embeddings and the movie embeddings\n",
    "    similarities = cosine_similarity(text_embeddings_np, g_only_embeddings)\n",
    "\n",
    "    # Find the indices of the embeddings that are similar to the intro and subtitles\n",
    "    intro_subtitle_indices = np.where(similarities.max(axis=0) > threshold)[0]\n",
    "\n",
    "    print(\"Number of images(Intro and Closing credits) to remove:\", intro_subtitle_indices)\n",
    "\n",
    "    # Remove the intro and subtitles from the movie embeddings\n",
    "    for idx in sorted(intro_subtitle_indices, reverse=True):\n",
    "        del g_movie_embeddings[idx]\n",
    "        g_only_embeddings = np.delete(g_only_embeddings, idx, axis=0)\n",
    "    \n",
    "\n",
    "# Global Variables\n",
    "\n",
    "g_movie_embeddings = json.load(open(\"honey_i_shrunk_the_kids_movie_embeddings_1_second.json\"))\n",
    "g_only_embeddings = np.array([emb['embedding'] for emb in g_movie_embeddings])\n",
    "find_and_remove_intro_and_subtitles(g_only_embeddings, threshold=0.237)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "\n",
    "We will compare clustering with t-SNE (t-Distributed Stochastic Neighbor Embedding) and PCA (Principal Component Analysis) dimensionality reduction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using t-SNE to embed the vectors into 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tSNE_embedded_vectors = tsne.fit_transform(g_only_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA to embed the vectors into 2D\n",
    "pca = PCA(n_components=2)\n",
    "PCA_embedded_vectors = pca.fit_transform(g_only_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster t-SNE and PCA with K-Means and display Silhoutte Score\n",
    "\n",
    "**Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the best clustering number for t-SNE and PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tSNE_embedded_vectors\n",
    "\n",
    "# Range of clusters to try\n",
    "num_clusters = range(2, 20)\n",
    "\n",
    "# List to hold silhouette scores\n",
    "sil_scores = []\n",
    "\n",
    "# Loop over number of clusters\n",
    "for k in num_clusters:\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_init=\"auto\", n_clusters=k, random_state=42).fit(X)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute silhouette score and append to list\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(num_clusters, sil_scores, 'bx-')\n",
    "plt.title('t-SNE')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X = PCA_embedded_vectors\n",
    "\n",
    "# Range of clusters to try\n",
    "num_clusters = range(2, 20)\n",
    "\n",
    "# List to hold silhouette scores\n",
    "sil_scores = []\n",
    "\n",
    "# Loop over number of clusters\n",
    "for k in num_clusters:\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_init=\"auto\", n_clusters=k, random_state=42).fit(X)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute silhouette score and append to list\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(num_clusters, sil_scores, 'bx-')\n",
    "plt.title('PCA')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets cluster with t-SNE and PCA best Silhoutte Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing KMeans clustering with best k silhoutte score.\n",
    "kmeans = KMeans(n_init=\"auto\", n_clusters=16, random_state=42)\n",
    "tSNE_clusters = kmeans.fit_predict(tSNE_embedded_vectors)\n",
    "\n",
    "kmeans = KMeans(n_init=\"auto\", n_clusters=4, random_state=42)\n",
    "PCA_clusters = kmeans.fit_predict(PCA_embedded_vectors)\n",
    "\n",
    "\n",
    "# Extracting numbers from file names for labels\n",
    "labels = [re.search(r'\\d+', vector['input']).group() for vector in g_movie_embeddings]\n",
    "\n",
    "#t-SNE\n",
    "# Plotting the embedded vectors with cluster coloring\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "sns.scatterplot(x=tSNE_embedded_vectors[:, 0], y=tSNE_embedded_vectors[:, 1], hue=tSNE_clusters, palette='bright', legend='full', s=100)\n",
    "for i, vec in enumerate(tSNE_embedded_vectors):\n",
    "    plt.text(vec[0] + 0.02, vec[1] + 0.02, labels[i], fontsize=6)  # Adding labels\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('t-SNE Embedded Vectors with KMeans Clustering (k=16)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "#PCA\n",
    "# Plotting the embedded vectors with cluster coloring\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "sns.scatterplot(x=PCA_embedded_vectors[:, 0], y=PCA_embedded_vectors[:, 1], hue=PCA_clusters, palette='bright', legend='full', s=100)\n",
    "for i, vec in enumerate(PCA_embedded_vectors):\n",
    "    plt.text(vec[0] + 0.02, vec[1] + 0.02, labels[i], fontsize=6)  # Adding labels\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('PCA Embedded Vectors with KMeans Clustering (k=4)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let sample images in t-SNE clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = set(tSNE_clusters)\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    display_cluster_images_first_last_x(tSNE_clusters, cluster, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets sample images in PCA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = set(PCA_clusters)\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    display_cluster_images_first_last_x(PCA_clusters, cluster, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cluster_images_first_last_x(tSNE_clusters, cluster, 192, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to timestamps by dividing by frame rate\n",
    "# Assuming `frame_rate` is the frame rate of the movie\n",
    "frame_rate = 24\n",
    "timestamps = [int(label) / frame_rate for label in labels]\n",
    "\n",
    "# Create a timeline plot for the t-SNE clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps, tSNE_clusters, c=tSNE_clusters, cmap='viridis')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster')\n",
    "plt.title('t-SNE Clusters Over Time')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Create a timeline plot for the PCA clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps, PCA_clusters, c=PCA_clusters, cmap='viridis')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster')\n",
    "plt.title('PCA Clusters Over Time')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Film Description\n",
    "#### Honey, I Shrunk the Kids\n",
    "1989 PG 1h 33m\n",
    "\n",
    "From the IMDB website:\n",
    "\n",
    "\"The scientist father of a teenage girl and boy accidentally shrinks his and two other neighborhood teens to the size of insects. Now the teens must fight diminutive dangers as the father searches for them.\"\n",
    "\n",
    "IMDB website. (n.d.). imdb.com. Retrieved April 27, 2024, from https://www.imdb.com/title/tt0097523/\n",
    "\n",
    "In 'Honey, I Shrunk the Kids,' an eccentric inventor, Wayne Szalinski, accidentally shrinks his and his neighbor's children with his experimental shrink ray. The miniature kids must navigate a perilous journey across their now-gigantic backyard, encountering obstacles like insects and sprinklers, as they try to return home.\n",
    "\n",
    "The film is notable for its creative visual effects that magnify ordinary environments into epic landscapes. It's a blend of adventure, humor, and family dynamics, ultimately showcasing the children's resourcefulness and the parents' determination to rescue their kids. The movie was a commercial success and spawned a franchise including sequels and a television series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Summary\n",
    "\n",
    "This section should highlight methods you used in your exploratory analysis. You should include at least one clustering technique or develop another way to relate frames to other frames. You should also consider dimensionality reduction.\n",
    "\n",
    "Each thumbnail is one frame of the Movie. Each thumbnail has been analyzed and embedded via CLIP. Referenced Model: https://replicate.com/andreasjansson/clip-features/examples .\n",
    "\n",
    "#### For hypothesis 1 we used the following methods\n",
    "\n",
    "1. Explore CLIP through a natural query for objects and investigate the surounding frames.\n",
    "2. Look at the Euclidean distance of similar frames.\n",
    "3. Explore similar frames for insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunches and Hypotheses\n",
    "\n",
    "This section should summarize the questions that you asked about the film that could potentially be answered by exploratory analysis. You should ask at least three questions.\n",
    "\n",
    "**Hypothesis 1** \n",
    "-   Using Clip to find an object and observe ~5 frames before and after and interpret the results.\n",
    "-   A hypothesis we discussed was that certain characters, or objects in the movie would be used for dramatic or comedic effect. We are using CLIP to identify these objects and then explore the surrounding frames to discover any evidence to support or refute this hypothesis.\n",
    "\n",
    "**Hypothesis 2** \n",
    "-   Scene Consistency and Transition - Frames that are visually and thematically similar cluster together tightly in t-SNE and PCA visualizations, and distinct clusters correspond to different scenes or settings in the movie.\n",
    "-  **Rationale:** This hypothesis tests the ability of CLIP embeddings, which capture both visual and semantic content, to differentiate between distinct scenes based on their visual content and thematic elements.\n",
    "\n",
    "**Hypothesis 3**\n",
    "-   Find a model that can analyze an image and identify how many objects are there. 2 trees, 1 dog, 3 humans, a house...etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Interpretation\n",
    "\n",
    "**Hypothesis 1:**\n",
    "- Begin by looking for the top images of a dog, lawnmower and an ant returned from CLIP queries.\n",
    "- Explore the frames surrounding the images identified by the above queries.\n",
    "\n",
    "These are the top matches returned by CLIP.\n",
    "\n",
    "1. 'a photo of a white and brown dog' = thumbnail_5039.jpg\n",
    "2. 'a photo of a lawnmower' = thumbnail_4734.jpg\n",
    "3. 'a photo of an ant' = thumbnail_3231.jpg\n",
    "\n",
    "The indexes we are interested in are 5038, 4733 and 3230.\n",
    "\n",
    "- For the dog image:\n",
    "\n",
    "From this sequence of frames, we see the children running, the parents talking, the dog looking up at the table, the kids waving their arms for attention and ends with the father narrowly avoiding accidentally eating his shrunken son. The dog being in the center anchors the dramatic tension of the scene. He almost looks like he is trying to warn the father or somehow knows more than the humans. Sentient animals with superior knowledge or wisdom than humans has been a trope used in several other films which depict animals (especially in comedies) as central figures.\n",
    "\n",
    "- For the lawnmower image:\n",
    "\n",
    "In this scene a neighbor kid is remotely controlling the lawnmower while the children are in the grass. This is a clear action scene with a threat elliciting dramatic flight.\n",
    "\n",
    "There doesn't seem to be many similar frames in the movie for the lawnmower. This is probably because it is a singular threat evnt used for dramatic tension in a single scene. It does look like it may appear around 790, so let's see if that is foreshadowing.\n",
    "\n",
    "In the scene around index 790 we see the son showing the remote controlled lawnmower to the boy who appears later controlling the mower in the flight scene. So this is an example of foreshadowing in a film that we identified by using the data to point us to something interesting!\n",
    "\n",
    "- For the ant image:\n",
    "\n",
    "In this scene, we see a boy easting some white substance, followed by a girl who appears scared or surprised. The next frame shows the cause, which is an ant menacing the kids, which is noticed by the boy as well, and the kids seem to begin fleeing the threat.\n",
    "\n",
    "The ant seems to be a later threat in the film, possibly also foreshadowed around 2180. However it appears to show up for an extended time around 4400.\n",
    "\n",
    "2180 looks like it matches the ant scene because of the setting. They both have extensive vegetation in the scene, which probably accounts for the lower distance than the later match around 4400.\n",
    "\n",
    "_**This identified the same scene that we discovered through exploration.**_\n",
    "\n",
    "We began with a hypothesis that the dog would be comedic relief and the ant and lawnmower would be for dramatic effect or threat of danger.We used CLIP to identify the inital frames of interest, then proceeded analyzing the target frames using the Euclidian distance, performing more exploration of similar frames and then circling back to a more specific CLIP question. The original hypothesis that these objects would be used as comedic relief or dramatic seems to have some justification, as the lawnmower and ant appear intially as dramatic threats, but from this discovery, it looks like the ant actually fought a scorpion and saved the kids, which indicated it was also used to elicit a sympathetic response from the audience. \n",
    "\n",
    "**Hypothesis 2:** \n",
    "- After removing closing credits frames and applying t-SNE and K-means we examined samples from identified clusters and we can observe that frames correspond to specific scenes or types of scenes (indoor vs outdoor, calm vs action-packed). Embeddings effectively capture scene-specific features and can be used to segment the movie based on visual content. Also, clusters seem to reflect the proper timeframe and transition of the movie.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Reflect on your process of analysis. What worked well and did not work well? Describe the limitations of the work and describe what you would work on with more time.\n",
    "\n",
    "We initially began exploring CLIP to discover the capabilities it provided. We were impressed with the model's ability to return results from simple english questions, so we brainstormed for some ideas that we could explore it as well as other tools to compare and contrast CLIP to other tools and also to discover what it could tell us about the film itself.\n",
    "\n",
    "We came up with the hypothesis that we could identify some objects in the film and explore whether they provided an assumed purpose in the movie. We did achieve (subjectively) a confirmation tthat the objects were used in a way that coincided with our hypothesis. What was interesting is the way that our initial exploration led to a secondary use that effectively used CLIP to identify a further area of research. This resulted in the surprising find that the ant seems to not only be used as a threat but also as an ally to evoke both fear and empathy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 1 Code and Results in Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_idx = 5038\n",
    "mower_idx = 4733\n",
    "ant_idx = 3230\n",
    "image_root = 'thumbnails_folder2large/'\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "with open(\"honey_i_shrunk_the_kids_movie_embeddings_1_second.json\", 'r') as file:\n",
    "    movie_embeddings = json.load(file)\n",
    "\n",
    "def euclidean_distance(array1, array2):\n",
    "    # Convert the arrays to NumPy arrays\n",
    "    array1_np = np.array(array1)\n",
    "    array2_np = np.array(array2)\n",
    "    \n",
    "    # Calculate the Euclidean distance\n",
    "    distance = np.linalg.norm(array1_np - array2_np)\n",
    "    return distance\n",
    "\n",
    "def find_and_display_matches(text_queries, top_k=5):\n",
    "    inputs = processor(text=text_queries, return_tensors=\"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    text_embeddings_np = text_embeddings.detach().numpy()\n",
    "    movie_embeddings_np = np.array([movie['embedding'] for movie in movie_embeddings])\n",
    "    similarities = cosine_similarity(text_embeddings_np, movie_embeddings_np)\n",
    "    for index, text_query in enumerate(text_queries):\n",
    "        print(f\"Top matches for: {text_query}\")\n",
    "        top_indices = np.argsort(similarities[index])[::-1][:top_k]\n",
    "        for i in top_indices:\n",
    "            frame = movie_embeddings[i]['input']\n",
    "            print(f\"Displaying frame: {frame}\")\n",
    "            display(Image(filename=f'thumbnails_folder2large/{frame}'))\n",
    "\n",
    "def plot_euclidean_distance_from(target_idx):\n",
    "    target = movie_embeddings[target_idx]\n",
    "    image_path = image_root+target[\"input\"]\n",
    "    # Display the image\n",
    "    display(Image(filename=image_path))\n",
    "\n",
    "    index_to_distance = []\n",
    "\n",
    "    # Iterate through the input list\n",
    "    for emb in movie_embeddings:\n",
    "        current_dist = euclidean_distance(emb[\"embedding\"], target[\"embedding\"])\n",
    "        index_to_distance.append(current_dist)\n",
    "\n",
    "    # Create a plot using Seaborn\n",
    "    sns.set(style=\"whitegrid\")  # Set the style\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    sns.lineplot(x=range(len(index_to_distance)), y=index_to_distance)  # Plot the array with index as x-axis\n",
    "    plt.xlabel(\"Index\")  # Set the x-axis label\n",
    "    plt.ylabel(\"Distance\")  # Set the y-axis label\n",
    "    plt.title(\"Distance from Target Over Film\")  # Set the title\n",
    "    plt.show()  # Show the plot\n",
    "\n",
    "def display_surrounding_frames(target_idx, frame_range=5):\n",
    "    idx_begin = target_idx - frame_range\n",
    "    idx_end = target_idx + frame_range + 1\n",
    "\n",
    "    display_frames = movie_embeddings[idx_begin:idx_end]\n",
    "    for emb in display_frames:\n",
    "        print(f'frame {emb[\"input\"]}')\n",
    "        image_path = image_root + emb[\"input\"]\n",
    "        display(Image(filename=image_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Dog\n",
    "Dislpay the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(dog_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_euclidean_distance_from(dog_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the following to explore 500, 2100 and 3400\n",
    "#display_surrounding_frames(3400, frame_range=40)\n",
    "# Discovered the dog in\n",
    "# thumbnail_0514.jpg\n",
    "# thumbnail_2078.jpg\n",
    "# thumbnail_2131.jpg\n",
    "# thumbnail_3427.jpg\n",
    "# Indexes are are 0 based\n",
    "display_surrounding_frames(513, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dog is watching the father's scientific experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(2077, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dog seems to notice something outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(2130, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parents are distracted, and the dog wants to investigate what is going on outside, once again exhibiting a higher sense of awareness than the humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(3426, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the dog is spinning the father around and disrupting his search for the children, which is an example of a comedic scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Lawnmower\n",
    "Dislpay the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(mower_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_euclidean_distance_from(mower_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display around 790\n",
    "#display_surrounding_frames(790, frame_range=40)\n",
    "display_surrounding_frames(790, frame_range=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Ant\n",
    "Dislpay the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(ant_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_euclidean_distance_from(ant_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display around 2180\n",
    "#display_surrounding_frames(2180, frame_range=40)\n",
    "display_surrounding_frames(2180, frame_range=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display around 4400\n",
    "#display_surrounding_frames(4400, frame_range=40)\n",
    "display_surrounding_frames(4435, frame_range=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scene around 4400 shows an ant fighting a scorpion and appearing to save the kids.\n",
    "\n",
    "Use CLIP to ask about 'a photo of an ant fighting a scorpion' to see what it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_query = ['a photo of an ant fighting a scorpion'] \n",
    "find_and_display_matches(exploration_query, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
