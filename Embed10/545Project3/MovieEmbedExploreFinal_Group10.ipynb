{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 545 M7 Project: Latent Space Cluster Analysis\n",
    "\n",
    "- Stace Worrell\n",
    "- Tochukwu \"Sylvester\" Nwizu\n",
    "- Giuseppe Schintu\n",
    "\n",
    "- Movie: Honey, I Shrunk The Kids!\n",
    "\n",
    "\n",
    "`In this project, we aim to address the following three hypotheses:`\n",
    "- Analyzing Narrative Impact: The Role of Key Characters and Objects in Film as Identified by CLIP\n",
    "- Scene Consistency and Transition: Frames that are visually and thematically similar cluster together tightly in t-SNE and PCA visualizations, and distinct clusters correspond to different scenes/settings in the movie.\n",
    "- Quantitative Analysis of Object Distribution in Images Using Deep Learning Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Code and Functions\n",
    "Run this first to import modules and global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from IPython.display import Image, display\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Models\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Global Functions\n",
    "\n",
    "def get_top50_ann(target_embedding, embeddings):\n",
    "    nn = NearestNeighbors(n_neighbors=51, metric='cosine', algorithm='brute')\n",
    "    nn.fit(embeddings)\n",
    "    distances, indices = nn.kneighbors([target_embedding])\n",
    "    return indices[0][1:]  # Skip the first index because it's the target itself\n",
    "\n",
    "def get_top50_euclidean(target_embedding, embeddings):\n",
    "    distances = [distance.euclidean(target_embedding, emb) for emb in embeddings]\n",
    "    indices = np.argsort(distances)[1:51]  # Skip the first index because it's the target itself\n",
    "    return indices\n",
    "\n",
    "def display_image(index):\n",
    "    display(Image(filename=f\"thumbnails_folder2large/{g_movie_embeddings[index]['input']}\"))\n",
    "\n",
    "def display_images(indices, embeddings):\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(20, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(plt.imread(f\"thumbnails_folder2large/{g_movie_embeddings[indices[i]]['input']}\"))\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images_first_x_last_x(indices, first_x, last_x, cluster_n=0):\n",
    "    # Select the first_x and last_x indices\n",
    "    selected_indices = indices[:first_x] + indices[-last_x:]\n",
    "    \n",
    "    # Calculate the number of rows and columns for the subplot\n",
    "    total_images = first_x + last_x\n",
    "    cols = 10\n",
    "    rows = math.ceil(total_images / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 2 * rows))\n",
    "    axes = axes.ravel()  # Flatten the axes array\n",
    "    \n",
    "    # Hide all axes\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Display images on the first len(selected_indices) axes\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        axes[i].imshow(plt.imread(f\"thumbnails_folder2large/{g_movie_embeddings[idx]['input']}\"))\n",
    "        axes[i].axis('on')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.title(f\"Cluster {cluster_n} -  First {first_x} and Last {last_x} Images - Total Images in Cluster: {len(indices)}\")\n",
    "    plt.show()\n",
    "\n",
    "def display_cluster_images(cluster_labels, cluster_number):\n",
    "    # Get indices of images in the cluster\n",
    "    indices = [i for i, label in enumerate(cluster_labels) if label == cluster_number]\n",
    "    \n",
    "    # Display images\n",
    "    display_images(indices)\n",
    "\n",
    "def display_cluster_images_first_last_x(cluster_labels, cluster_number, first_x, last_x):\n",
    "    # Get indices of images in the cluster\n",
    "    indices = [i for i, label in enumerate(cluster_labels) if label == cluster_number]\n",
    "    \n",
    "    # Display images\n",
    "    display_images_first_x_last_x(indices, first_x, last_x, cluster_number)\n",
    "\n",
    "def find_and_remove_intro_and_subtitles(g_only_embeddings, threshold=0.7):\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # Get the text embeddings for the intro and subtitles\n",
    "    inputs = processor(text=[\"Image of Walt Disney Movie Intro\", \"Walt Disney Movie Intro\", \"Movie closing credits\", \"Movie end credits\", \"Image with lots of closing credits\"], return_tensors=\"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    text_embeddings_np = text_embeddings.detach().numpy()\n",
    "\n",
    "    # Calculate the cosine similarity between the text embeddings and the movie embeddings\n",
    "    similarities = cosine_similarity(text_embeddings_np, g_only_embeddings)\n",
    "\n",
    "    # Find the indices of the embeddings that are similar to the intro and subtitles\n",
    "    intro_subtitle_indices = np.where(similarities.max(axis=0) > threshold)[0]\n",
    "\n",
    "    #print(\"Number of images(Intro and Closing credits) to remove:\", intro_subtitle_indices)\n",
    "\n",
    "    # Create new lists that exclude the intro and subtitles\n",
    "    new_g_movie_embeddings = [emb for i, emb in enumerate(g_movie_embeddings) if i not in intro_subtitle_indices]\n",
    "    new_g_only_embeddings = np.array([emb for i, emb in enumerate(g_only_embeddings) if i not in intro_subtitle_indices])\n",
    "\n",
    "    return new_g_movie_embeddings, new_g_only_embeddings\n",
    "\n",
    "# Global Variables\n",
    "g_movie_embeddings = json.load(open(\"honey_i_shrunk_the_kids_movie_embeddings_1_second.json\"))\n",
    "g_only_embeddings = np.array([emb['embedding'] for emb in g_movie_embeddings])\n",
    "\n",
    "g_movie_embeddings, g_only_embeddings = find_and_remove_intro_and_subtitles(g_only_embeddings, threshold=0.237)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_idx = 5038\n",
    "mower_idx = 4733\n",
    "ant_idx = 3230\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "with open(\"honey_i_shrunk_the_kids_movie_embeddings_1_second.json\", 'r') as file:\n",
    "    movie_embeddings = json.load(file)\n",
    "\n",
    "def euclidean_distance(array1, array2):\n",
    "    # Convert the arrays to NumPy arrays\n",
    "    array1_np = np.array(array1)\n",
    "    array2_np = np.array(array2)\n",
    "    \n",
    "    # Calculate the Euclidean distance\n",
    "    distance = np.linalg.norm(array1_np - array2_np)\n",
    "    return distance\n",
    "\n",
    "def find_and_display_matches(text_queries, top_k=5):\n",
    "    inputs = processor(text=text_queries, return_tensors=\"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    text_embeddings_np = text_embeddings.detach().numpy()\n",
    "    movie_embeddings_np = np.array([movie['embedding'] for movie in movie_embeddings])\n",
    "    similarities = cosine_similarity(text_embeddings_np, movie_embeddings_np)\n",
    "    for index, text_query in enumerate(text_queries):\n",
    "        print(f\"Top matches for: {text_query}\")\n",
    "        top_indices = np.argsort(similarities[index])[::-1][:top_k]\n",
    "        for i in top_indices:\n",
    "            frame = movie_embeddings[i]['input']\n",
    "            print(f\"Displaying frame: {frame}\")\n",
    "            display(Image(filename=f'thumbnails_folder2large/{frame}'))\n",
    "\n",
    "def plot_euclidean_distance_from(target_idx):\n",
    "    target = movie_embeddings[target_idx]\n",
    "    image_path = image_path = 'thumbnails_folder2large/' + target[\"input\"]\n",
    "    # Display the image\n",
    "    display(Image(filename=image_path))\n",
    "\n",
    "    index_to_distance = []\n",
    "\n",
    "    # Iterate through the input list\n",
    "    for emb in movie_embeddings:\n",
    "        current_dist = euclidean_distance(emb[\"embedding\"], target[\"embedding\"])\n",
    "        index_to_distance.append(current_dist)\n",
    "\n",
    "    # Create a plot using Seaborn\n",
    "    sns.set(style=\"whitegrid\")  # Set the style\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    sns.lineplot(x=range(len(index_to_distance)), y=index_to_distance)  # Plot the array with index as x-axis\n",
    "    plt.xlabel(\"Index\")  # Set the x-axis label\n",
    "    plt.ylabel(\"Distance\")  # Set the y-axis label\n",
    "    plt.title(\"Distance from Target Over Film\")  # Set the title\n",
    "    plt.show()  # Show the plot\n",
    "\n",
    "def display_surrounding_frames(target_idx, frame_range=5):\n",
    "    idx_begin = target_idx - frame_range\n",
    "    idx_end = target_idx + frame_range + 1\n",
    "\n",
    "    display_frames = movie_embeddings[idx_begin:idx_end]\n",
    "    for emb in display_frames:\n",
    "        print(f'frame {emb[\"input\"]}')\n",
    "        image_path = 'thumbnails_folder2large/' + emb[\"input\"]\n",
    "        display(Image(filename=image_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Dog\n",
    "Dislpay the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(dog_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_euclidean_distance_from(dog_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the following to explore 500, 2100 and 3400\n",
    "#display_surrounding_frames(3400, frame_range=40)\n",
    "# Discovered the dog in\n",
    "# thumbnail_0514.jpg\n",
    "# thumbnail_2078.jpg\n",
    "# thumbnail_2131.jpg\n",
    "# thumbnail_3427.jpg\n",
    "# Indexes are are 0 based\n",
    "display_surrounding_frames(513, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dog is watching the father's scientific experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(2077, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dog seems to notice something outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(2130, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parents are distracted, and the dog wants to investigate what is going on outside, once again exhibiting a higher sense of awareness than the humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(3426, frame_range=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the dog is spinning the father around and disrupting his search for the children, which is an example of a comedic scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Lawnmower\n",
    "Dislpay the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(mower_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_euclidean_distance_from(mower_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display around 790\n",
    "#display_surrounding_frames(790, frame_range=40)\n",
    "display_surrounding_frames(790, frame_range=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Ant\n",
    "Dislpay the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_surrounding_frames(ant_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_euclidean_distance_from(ant_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display around 2180\n",
    "#display_surrounding_frames(2180, frame_range=40)\n",
    "display_surrounding_frames(2180, frame_range=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display around 4400\n",
    "#display_surrounding_frames(4400, frame_range=40)\n",
    "display_surrounding_frames(4435, frame_range=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scene around 4435 shows an ant fighting a scorpion and appearing to save the kids.\n",
    "\n",
    "Use CLIP to ask about 'a photo of an ant fighting a scorpion' to see what it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_query = ['a photo of an ant fighting a scorpion'] \n",
    "find_and_display_matches(exploration_query, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 2\n",
    "### - Scene Consistency and Transition with dimensionality reduction and clustering -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "\n",
    "We will compare clustering with t-SNE (t-Distributed Stochastic Neighbor Embedding) and PCA (Principal Component Analysis) dimensionality reduction algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using t-SNE to embed the vectors into 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tSNE_embedded_vectors = tsne.fit_transform(g_only_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA to embed the vectors into 2D\n",
    "pca = PCA(n_components=2)\n",
    "PCA_embedded_vectors = pca.fit_transform(g_only_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster t-SNE and PCA with K-Means and display Silhoutte Score\n",
    "\n",
    "**Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the best clustering number for t-SNE and PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tSNE_embedded_vectors\n",
    "\n",
    "# Range of clusters to try\n",
    "num_clusters = range(2, 20)\n",
    "\n",
    "# List to hold silhouette scores\n",
    "sil_scores = []\n",
    "\n",
    "# Loop over number of clusters\n",
    "for k in num_clusters:\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_init=\"auto\", n_clusters=k, random_state=42).fit(X)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute silhouette score and append to list\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(num_clusters, sil_scores, 'bx-')\n",
    "plt.title('t-SNE')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X = PCA_embedded_vectors\n",
    "\n",
    "# Range of clusters to try\n",
    "num_clusters = range(2, 20)\n",
    "\n",
    "# List to hold silhouette scores\n",
    "sil_scores = []\n",
    "\n",
    "# Loop over number of clusters\n",
    "for k in num_clusters:\n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_init=\"auto\", n_clusters=k, random_state=42).fit(X)\n",
    "    \n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Compute silhouette score and append to list\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(num_clusters, sil_scores, 'bx-')\n",
    "plt.title('PCA')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets cluster with t-SNE and PCA best Silhoutte Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing KMeans clustering with best k silhoutte score.\n",
    "kmeans = KMeans(n_init=\"auto\", n_clusters=19, random_state=42)\n",
    "tSNE_clusters = kmeans.fit_predict(tSNE_embedded_vectors)\n",
    "\n",
    "kmeans = KMeans(n_init=\"auto\", n_clusters=4, random_state=42)\n",
    "PCA_clusters = kmeans.fit_predict(PCA_embedded_vectors)\n",
    "\n",
    "\n",
    "# Extracting numbers from file names for labels\n",
    "labels = [re.search(r'\\d+', vector['input']).group() for vector in g_movie_embeddings]\n",
    "\n",
    "#t-SNE\n",
    "# Plotting the embedded vectors with cluster coloring\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "sns.scatterplot(x=tSNE_embedded_vectors[:, 0], y=tSNE_embedded_vectors[:, 1], hue=tSNE_clusters, palette='bright', legend='full', s=100)\n",
    "for i, vec in enumerate(tSNE_embedded_vectors):\n",
    "    plt.text(vec[0] + 0.02, vec[1] + 0.02, labels[i], fontsize=6)  # Adding labels\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('t-SNE Embedded Vectors with KMeans Clustering (k=16)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "#PCA\n",
    "# Plotting the embedded vectors with cluster coloring\n",
    "sns.set_theme()\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
    "sns.scatterplot(x=PCA_embedded_vectors[:, 0], y=PCA_embedded_vectors[:, 1], hue=PCA_clusters, palette='bright', legend='full', s=100)\n",
    "for i, vec in enumerate(PCA_embedded_vectors):\n",
    "    plt.text(vec[0] + 0.02, vec[1] + 0.02, labels[i], fontsize=6)  # Adding labels\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('PCA Embedded Vectors with KMeans Clustering (k=4)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let sample images in t-SNE clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = set(tSNE_clusters)\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    display_cluster_images_first_last_x(tSNE_clusters, cluster, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets sample images in PCA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = set(PCA_clusters)\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    display_cluster_images_first_last_x(PCA_clusters, cluster, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE and PCA with K-means clusters over a timeline\n",
    "By looking at these plots, we can see if frames from the same cluster tend to occur close together in time, which might indicate that the clustering is capturing some meaningful structure in the movie. For example, all the frames from a particular scene might be grouped into the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to timestamps by dividing by frame rate\n",
    "# Assuming `frame_rate` is the frame rate of the movie\n",
    "frame_rate = 24\n",
    "timestamps = [int(label) / frame_rate for label in labels]\n",
    "\n",
    "# Create a timeline plot for the t-SNE clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps, tSNE_clusters, c=tSNE_clusters, cmap='viridis')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster')\n",
    "plt.title('t-SNE Clusters Over Time')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Create a timeline plot for the PCA clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps, PCA_clusters, c=PCA_clusters, cmap='viridis')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster')\n",
    "plt.title('PCA Clusters Over Time')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3\n",
    "\n",
    "### Find a model that can identify objects from movie frame images\n",
    "#### Utilizing the Yolo model for recognizing and counting objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n",
    "\n",
    "\n",
    "image_directory = \"thumbnails_folder2large/\"\n",
    "\n",
    "# List the first 100 images in the image file directory\n",
    "image_files = [\n",
    "    f\n",
    "    for f in os.listdir(image_directory)\n",
    "    if os.path.isfile(os.path.join(image_directory, f))\n",
    "]\n",
    "image_files = image_files[:100]\n",
    "\n",
    "# Initialize a Counter to keep track of object counts\n",
    "total_counts = Counter()\n",
    "\n",
    "\n",
    "for image_file in image_files:\n",
    "\n",
    "    image_path = os.path.join(image_directory, image_file)\n",
    "\n",
    "    results = model(image_path)\n",
    "\n",
    "    print(f\"Results for {image_file}:\")\n",
    "    df = results.pandas().xyxy[0]  # Results as DataFrame\n",
    "    print(df)\n",
    "\n",
    "    # Update the total counts of objects\n",
    "    counts = df[\"name\"].value_counts()\n",
    "    total_counts.update(counts)\n",
    "\n",
    "    results.show()\n",
    "    results.save(save_dir=\"output/\")\n",
    "\n",
    "    object_counts = df[\"name\"].value_counts()\n",
    "    print(\"Object counts:\", object_counts)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total counts of detected objects:\")\n",
    "for object_type, count in total_counts.items():\n",
    "    print(f\"{object_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Film Description\n",
    "#### Honey, I Shrunk the Kids\n",
    "1989 PG 1h 33m\n",
    "\n",
    "From the IMDB website:\n",
    "\n",
    "\"The scientist father of a teenage girl and boy accidentally shrinks his and two other neighborhood teens to the size of insects. Now the teens must fight diminutive dangers as the father searches for them.\"\n",
    "\n",
    "IMDB website. (n.d.). imdb.com. Retrieved April 27, 2024, from https://www.imdb.com/title/tt0097523/\n",
    "\n",
    "In 'Honey, I Shrunk the Kids,' an eccentric inventor, Wayne Szalinski, accidentally shrinks his and his neighbor's children with his experimental shrink ray. The miniature kids must navigate a perilous journey across their now-gigantic backyard, encountering obstacles like insects and sprinklers, as they try to return home.\n",
    "\n",
    "The film is notable for its creative visual effects that magnify ordinary environments into epic landscapes. It's a blend of adventure, humor, and family dynamics, ultimately showcasing the children's resourcefulness and the parents' determination to rescue their kids. The movie was a commercial success and spawned a franchise including sequels and a television series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Summary\n",
    "\n",
    "#### For hypothesis 1 we used the following methods\n",
    "\n",
    "1. Explore CLIP through a natural query for objects and investigate the surounding frames.\n",
    "2. Look at the Euclidean distance of similar frames.\n",
    "3. Explore similar frames for insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunches and Hypotheses\n",
    "\n",
    "**Hypothesis 1** \n",
    "-   Analyzing Narrative Impact: The Role of Key Characters and Objects in Film as Identified by CLIP\n",
    "-   **Rationale:** In movies, the appearance of certain characters or objects is significantly associated with specific narrative effects such as humor or drama. By using the CLIP model to detect these elements in selected frames and analyzing the content of approximately five frames before and after their appearance, we can identify patterns that support or refute their role in contributing to these narrative effects.\n",
    "\n",
    "**Hypothesis 2** \n",
    "-   Scene Consistency and Transition: Frames that are visually and thematically similar cluster together tightly in t-SNE and PCA visualizations, and distinct clusters correspond to different scenes or settings in the movie.\n",
    "-  **Rationale:** This hypothesis tests the ability of CLIP embeddings, which capture both visual and semantic content, to differentiate between distinct scenes based on their visual content and thematic elements.\n",
    "\n",
    "**Hypothesis 3**\n",
    "-   Quantitative Analysis of Object Distribution in Images Using Deep Learning Models \n",
    "-   **Rationale:** The use of advanced image recognition models enables accurate identification and quantification of different object types within images, facilitating detailed analysis of object distribution patterns across varied datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Interpretation\n",
    "\n",
    "#### Hypothesis 1:\n",
    "- **Objective:** Identify and analyze the top images of a dog, a lawnmower, and an ant using CLIP queries, and explore the surrounding frames to determine their narrative impact.\n",
    "\n",
    "**Top Matches from CLIP:**\n",
    "1. **Dog:** 'a photo of a white and brown dog' - **thumbnail_5039.jpg**\n",
    "2. **Lawnmower:** 'a photo of a lawnmower' - **thumbnail_4734.jpg**\n",
    "3. **Ant:** 'a photo of an ant' - **thumbnail_3231.jpg**\n",
    "\n",
    "**Analysis of Key Frames:**\n",
    "- **Dog Scene Analysis:**\n",
    "  From the sequence of frames, the storyline unfolds with children running, parents talking, and the dog focusing upwards, climaxing with a dramatic moment where the father nearly harms his accidentally shrunken son. The dog's central position and alert demeanor suggest an awareness surpassing the humans, a trope often used to enhance dramatic tension in films.\n",
    "  \n",
    "- **Lawnmower Scene Analysis:**\n",
    "  A neighborhood child remotely controls the lawnmower amidst children playing on the grass, creating a direct threat in this action-packed scene. The foreshadowing of this event is noted at index 790, where the lawnmower and its controller are introduced, demonstrating the effective use of foreshadowing discovered through our frame-by-frame analysis.\n",
    "\n",
    "- **Ant Scene Analysis:**\n",
    "  Initially a threat, the ant later interacts with a scorpion, shifting its role from antagonist to protector. This transformation not only adds complexity to the character but also alters its narrative impact, from eliciting fear to evoking sympathy among the audience.\n",
    "\n",
    "#### Hypothesis 2:\n",
    "- **Objective:** Utilize t-SNE and K-means clustering to analyze and categorize movie frames by scene content and type after removing noise from non-relevant frames like opening and closing credits.\n",
    "\n",
    "**Findings:**\n",
    "  The embeddings captured by our models effectively differentiated between scene types (e.g., indoor vs. outdoor, calm vs. action-packed), helping segment the movie based on visual content. This clustering accurately reflected the proper timeframe and transitions within the movie, with t-SNE outperforming PCA in visual correlation and cluster separation.\n",
    "\n",
    "#### Hypothesis 3:\n",
    "- **Objective:** Analyze the first 100 images in the thumbnail folder using the Yolo model to identify and count types of objects, examining the effectiveness and efficiency of object detection in a controlled set.\n",
    "\n",
    "**Results:**\n",
    "  The Yolo model identified objects within the images, though processing was slow. The results were visualized using bounding boxes and documented in a DataFrame, summarizing the count and type of each detected object. This provided a clear, quantitative insight into the object distribution within the sampled frames.\n",
    "\n",
    "### Reflection\n",
    "\n",
    "**Process Review:**\n",
    "Our exploration of CLIP highlighted its robust capability to interpret natural language queries and direct our analytical focus, while integrating Yolo and clustering algorithms offered a multi-faceted view of the film's visual narrative.\n",
    "\n",
    "**Challenges and Limitations:**\n",
    "The slow processing speed of the full image analysis using Yolo was a notable limitation. Optimizing this aspect would be a primary goal for future work, alongside exploring additional AI tools for more nuanced insights.\n",
    "\n",
    "**Future Directions:**\n",
    "Given additional time, we would expand our analysis to include a larger dataset of films to validate and refine our findings further. Employing more advanced models could also uncover deeper insights into narrative structures and character roles within broader cinematic contexts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
